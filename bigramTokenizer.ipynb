{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ares Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\miguel granados c\\anaconda3\\envs\\equinox_base\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.2\n",
      "    Uninstalling pydantic-1.10.2:\n",
      "      Successfully uninstalled pydantic-1.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'C:\\\\Users\\\\Miguel Granados C\\\\anaconda3\\\\envs\\\\equinox_base\\\\Lib\\\\site-packages\\\\~-dantic\\\\annotated_types.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESCRIPCION</th>\n",
       "      <th>CODIGO_ETAPA</th>\n",
       "      <th>DURACION_HORAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vacaciones</td>\n",
       "      <td>VAC</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vacaciones</td>\n",
       "      <td>VAC</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Documentación proceso multipago bancario ajust...</td>\n",
       "      <td>COCOD</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Documentación proceso acumulación pisos y paso...</td>\n",
       "      <td>COCOD</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Documentación proceso acumulación pisos y cont...</td>\n",
       "      <td>COCOD</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vacaciones</td>\n",
       "      <td>VAC</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vacaciones</td>\n",
       "      <td>VAC</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vacaciones</td>\n",
       "      <td>VAC</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vacaciones</td>\n",
       "      <td>VAC</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vacaciones</td>\n",
       "      <td>VAC</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DESCRIPCION CODIGO_ETAPA  \\\n",
       "0                                         Vacaciones          VAC   \n",
       "1                                         Vacaciones          VAC   \n",
       "2  Documentación proceso multipago bancario ajust...        COCOD   \n",
       "3  Documentación proceso acumulación pisos y paso...        COCOD   \n",
       "4  Documentación proceso acumulación pisos y cont...        COCOD   \n",
       "5                                         Vacaciones          VAC   \n",
       "6                                         Vacaciones          VAC   \n",
       "7                                         Vacaciones          VAC   \n",
       "8                                         Vacaciones          VAC   \n",
       "9                                         Vacaciones          VAC   \n",
       "\n",
       "   DURACION_HORAS  \n",
       "0             8.0  \n",
       "1             8.0  \n",
       "2             3.0  \n",
       "3             3.0  \n",
       "4             2.0  \n",
       "5             8.0  \n",
       "6             8.0  \n",
       "7             8.0  \n",
       "8             8.0  \n",
       "9             8.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ARES2_EJECUCION_ACTIVIDADES.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                              DESCRIPCION CODIGO_ETAPA  \\\n",
       "0                                             Vacaciones          VAC   \n",
       "1                                             Vacaciones          VAC   \n",
       "2      Documentación proceso multipago bancario ajust...        COCOD   \n",
       "3      Documentación proceso acumulación pisos y paso...        COCOD   \n",
       "4      Documentación proceso acumulación pisos y cont...        COCOD   \n",
       "...                                                  ...          ...   \n",
       "52851  Reunión daily, Documentando el código desarrol...        COCOD   \n",
       "52852  Generando los test unitarios a los servicios y...        COCOD   \n",
       "52853  Homologando las colecciones de postman, optimi...        COCOD   \n",
       "52854  Validación de los datos de parametrización e i...        TRCON   \n",
       "52855  Apoyo al equipo de trabajo con las tareas del ...        DIAPL   \n",
       "\n",
       "       DURACION_HORAS  \n",
       "0                 8.0  \n",
       "1                 8.0  \n",
       "2                 3.0  \n",
       "3                 3.0  \n",
       "4                 2.0  \n",
       "...               ...  \n",
       "52851             3.0  \n",
       "52852             2.0  \n",
       "52853             3.0  \n",
       "52854             2.0  \n",
       "52855             2.0  \n",
       "\n",
       "[52856 rows x 3 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CODIGO_ETAPA'].nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "APSEG       1909\n",
    "PRSIS       1687\n",
    "ASEJE       1395\n",
    "COAJU       1370\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DESCRIPCION', 'CODIGO_ETAPA', 'DURACION_HORAS'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter the table in order to have only data from the some codes\n",
    "codes = ['APSEG', 'PRSIS', 'ASEJE', 'COAJU']\n",
    "df1=df[df[\"CODIGO_ETAPA\"].isin(codes)]\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spanish language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Documentación proceso', 'proceso multipago', 'multipago bancario', 'bancario ajuste', 'ajuste estructura', 'estructura archivo', 'archivo consignante']\n"
     ]
    }
   ],
   "source": [
    "# Define the function to tokenize a string into bigrams\n",
    "def tokenize_bigrams(text):\n",
    "    # Process the text using the spaCy pipeline\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemmatized tokens from the processed text\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Combine adjacent pairs of tokens into bigrams\n",
    "    bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "    return bigrams\n",
    "\n",
    "print(tokenize_bigrams(\"Documentación proceso multipago bancario ajuste estructura archivo consignantes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel Granados C\\AppData\\Local\\Temp\\ipykernel_5980\\124412619.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"bigrams\"] = df1[\"DESCRIPCION\"].apply(lambda x: set(tokenize_bigrams(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended CODIGO_ETAPA for this description is: APSEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Miguel Granados C\\anaconda3\\envs\\equinox_base\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the Spanish language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Tokenize the 'DESCRIPCION' column into bigrams and create a set of unique bigrams for each row\n",
    "df1[\"bigrams\"] = df1[\"DESCRIPCION\"].apply(lambda x: set(tokenize_bigrams(x)))\n",
    "\n",
    "# Define the function to tokenize a string into bigrams\n",
    "def tokenize_bigrams(text):\n",
    "    # Process the text using the spaCy pipeline\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemmatized tokens from the processed text\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Combine adjacent pairs of tokens into bigrams\n",
    "    bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "    return bigrams\n",
    "\n",
    "#Daily CTO bugs priorizados Fase II y Fase I\n",
    "\n",
    "# Prompt the user for a description\n",
    "user_description = input(\"Please enter a description: \")\n",
    "\n",
    "# Tokenize the user description into bigrams\n",
    "user_bigrams = set(tokenize_bigrams(user_description))\n",
    "\n",
    "# Create a bag of words from the unique bigrams in the dataset\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(), lowercase=False)\n",
    "corpus = [\" \".join(row) for row in df1[\"bigrams\"].values]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute the cosine similarity between the user bigrams and the bigrams in the dataset\n",
    "user_bow = vectorizer.transform([\" \".join(user_bigrams)])\n",
    "similarity_scores = cosine_similarity(user_bow, X)\n",
    "\n",
    "# Get the index of the row with the highest similarity score\n",
    "most_similar_index = similarity_scores.argmax()\n",
    "\n",
    "# Get the corresponding value of the 'CODIGO_ETAPA' column\n",
    "recommended_etapa = df1.iloc[most_similar_index][\"CODIGO_ETAPA\"]\n",
    "\n",
    "# Print the recommended CODIGO_ETAPA\n",
    "print(f\"The recommended CODIGO_ETAPA for this description is: {recommended_etapa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel Granados C\\AppData\\Local\\Temp\\ipykernel_5980\\1636091927.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"bigrams\"] = df1[\"DESCRIPCION\"].apply(lambda x: set(tokenize_bigrams(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended CODIGO_ETAPA for this description is: APSEG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the Spanish language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Tokenize the 'DESCRIPCION' column into bigrams and create a set of unique bigrams for each row\n",
    "df1[\"bigrams\"] = df1[\"DESCRIPCION\"].apply(lambda x: set(tokenize_bigrams(x)))\n",
    "\n",
    "# Define the function to tokenize a string into bigrams\n",
    "def tokenize_bigrams(text):\n",
    "    # Process the text using the spaCy pipeline\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemmatized tokens from the processed text\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Combine adjacent pairs of tokens into bigrams\n",
    "    bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "#Daily CTO bugs priorizados Fase II y Fase I\n",
    "\n",
    "# Prompt the user for a description\n",
    "user_description = input(\"Please enter a description: \")\n",
    "\n",
    "# Tokenize the user description into bigrams\n",
    "user_bigrams = set(tokenize_bigrams(user_description))\n",
    "\n",
    "# Create a bag of words from the unique bigrams in the dataset\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(), lowercase=False)\n",
    "corpus = [\" \".join(row) for row in df1[\"bigrams\"].values]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute the cosine similarity between the user bigrams and the bigrams in the dataset\n",
    "user = vectorizer.transform([\" \".join(user_bigrams)])\n",
    "similarity_scores = cosine_similarity(user, X)\n",
    "\n",
    "# Get the index of the row with the highest similarity score\n",
    "most_similar_index = similarity_scores.argmax()\n",
    "\n",
    "# Get the corresponding value of the 'CODIGO_ETAPA' column\n",
    "recommended_etapa = df1.iloc[most_similar_index][\"CODIGO_ETAPA\"]\n",
    "\n",
    "# Print the recommended CODIGO_ETAPA\n",
    "print(f\"The recommended CODIGO_ETAPA for this description is: {recommended_etapa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODIGO_ETAPA: APSEG - Jaccard similarity: 0.00\n",
      "CODIGO_ETAPA: PRSIS - Jaccard similarity: 0.00\n",
      "CODIGO_ETAPA: COAJU - Jaccard similarity: 0.00\n",
      "CODIGO_ETAPA: ASEJE - Jaccard similarity: 0.00\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the function to tokenize a string into bigrams\n",
    "def tokenize_bigrams(text):\n",
    "    # Process the text using the spaCy pipeline\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemmatized tokens from the processed text\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Combine adjacent pairs of tokens into bigrams\n",
    "    bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "    return set(bigrams)\n",
    "\n",
    "\n",
    "# Group the descriptions by unique code in 'CODIGO_ETAPA' and tokenize them into sets of bigrams\n",
    "code_sets = {}\n",
    "for code in df1['CODIGO_ETAPA'].unique():\n",
    "    code_set = set()\n",
    "    for description in df1.loc[df1['CODIGO_ETAPA'] == code, 'DESCRIPCION']:\n",
    "        code_set.update(tokenize_bigrams(description))\n",
    "    code_sets[code] = code_set\n",
    "\n",
    "# Get the user input\n",
    "prompt = input(\"Please enter a description: \")\n",
    "prompt_set = tokenize_bigrams(prompt)\n",
    "\n",
    "# Compute the Jaccard similarity between the sets and the prompt\n",
    "similarities = {}\n",
    "for code, code_set in code_sets.items():\n",
    "    jaccard_sim = len(prompt_set.intersection(code_set)) / len(prompt_set.union(code_set))\n",
    "    similarities[code] = jaccard_sim\n",
    "\n",
    "# Print the similarity scores in descending order\n",
    "for code, sim in sorted(similarities.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"CODIGO_ETAPA: {code} - Jaccard similarity: {sim:.2f}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equinox_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
